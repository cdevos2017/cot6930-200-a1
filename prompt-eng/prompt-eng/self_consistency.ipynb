{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Consistency Prompting\n",
    "\n",
    "One of the more advanced techniques in prompt engineering is self-consistency, introduced by `Wang et al. (2022)`. \n",
    "\n",
    "This method seeks to improve upon the traditional greedy decoding typically used in chain-of-thought (CoT) prompting. \n",
    "\n",
    "The core concept involves sampling multiple diverse reasoning paths through few-shot CoT and leveraging these variations to determine the most consistent answer. The technique  enhances the effectiveness of CoT prompting, particularly for tasks requiring arithmetic and commonsense reasoning.\n",
    "\n",
    "## References:\n",
    "* [Wang et al. (2022)](https://arxiv.org/abs/2203.11171)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running this code on MyBind.org\n",
    "\n",
    "Note: remember that you will need to **adjust CONFIG** with **proper URL and API_KEY**!\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/GenILab-FAU/prompt-eng/HEAD?urlpath=%2Fdoc%2Ftree%2Fprompt-eng%2Fself_consistency.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.2:latest', 'prompt': 'What is 984 * log(2)', 'stream': False, 'options': {'temperature': 1.0, 'num_ctx': 100, 'num_predict': 100}}\n",
      "To calculate this, we need to follow the order of operations (PEMDAS):\n",
      "\n",
      "1. Calculate the logarithm of 2:\n",
      "   log(2) ≈ 0.30103\n",
      "2. Multiply 984 by the result:\n",
      "\n",
      "984 × 0.30103 ≈ 296.95\n",
      "Time taken: 1.027s\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## ZERO SHOT PROMPTING\n",
    "##\n",
    "\n",
    "from _pipeline import create_payload, model_req\n",
    "\n",
    "#### (1) Adjust the inbounding  Prompt, simulating inbounding requests from users or other systems\n",
    "MESSAGE = \"What is 984 * log(2)\"\n",
    "\n",
    "#### (2) Adjust the Prompt Engineering Technique to be applied, simulating Workflow Templates\n",
    "\n",
    "## @TODO \n",
    "PROMPT = MESSAGE \n",
    "\n",
    "#### (3) Configure the Model request, simulating Workflow Orchestration\n",
    "# Documentation: https://github.com/ollama/ollama/blob/main/docs/api.md\n",
    "payload = create_payload(target=\"ollama\",\n",
    "                         model=\"llama3.2:latest\", \n",
    "                         prompt=PROMPT, \n",
    "                         temperature=1.0, \n",
    "                         num_ctx=100, \n",
    "                         num_predict=100)\n",
    "\n",
    "### YOU DONT NEED TO CONFIGURE ANYTHING ELSE FROM THIS POINT\n",
    "# Send out to the model\n",
    "time, response = model_req(payload=payload)\n",
    "print(response)\n",
    "if time: print(f'Time taken: {time}s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
