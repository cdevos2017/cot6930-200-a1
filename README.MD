![GenI-banner](https://github.com/genilab-fau/genilab-fau.github.io/blob/8d6ab41403b853a273983e4c06a7e52229f43df5/images/genilab-banner.png?raw=true)

# **Comparison of Prompt Refinement and Temperature Optimization Techniques for Various Requirements Analysis Tasks**

## **Project Overview**
This project is an **extension** of the Prompt Engineering Lab originally provided in the course. It expands the foundational framework by implementing **multi-level prompt engineering techniques** with a systematic **prompt refinement system** that improves requirements analysis tasks using **dynamic template generation, automated prompt refinement, and parameter tuning.** The research investigates how **Level-1 and Level-2 techniques** impact quality, consistency, and efficiency in **requirements extraction and analysis.**  

This project goes **beyond** the base implementation by:
- **Developing new techniques** for prompt refinement.
- **Experimenting with different parameters** to optimize results.
- **Performing an extensive comparative analysis** on technique effectiveness.
- **Automating prompt refinement and iterative evaluation.**
- **Generating empirical evidence** on prompt behavior across different **requirement types.**  
 
This work is based on the **Prompt Engineering Lab project** from:  
üìå **[Original Prompt Engineering Lab Repository](https://github.com/genilab-fau/prompt-eng)**  

---

## **Authors**
- **Chad Devos, M.S.** ([GitHub](https://github.com/cdevos2017))
- **Supervised by:** [Dr. Fernando Koch](http://www.fernandokoch.me)  
- **Affiliation:** Generative Intelligence Lab, FAU  

## üìë Research Report

The full research paper detailing the comparative study and findings can be found [here](research/research_report.pdf).

---

# **Research Question**
**How can multi-level prompt engineering techniques improve the quality, comprehensiveness, and consistency of requirements extracted from natural language descriptions?**  

---

## **Arguments & Research Goals**  

### **What is already known about this topic?**  
- Prompt engineering influences **output quality** but often requires **expert-level tuning** for best results.
- **Chain-of-thought, tree-of-thought, and self-consistency** methods can improve responses but are **task-dependent.**
- **Parameter tuning (temperature, context window, and length constraints)** affects consistency.
- Requirements analysis **is inherently ambiguous** and requires **iterative refinement** to achieve clarity.
- **Large Language Models (LLMs)** can automate requirements extraction **if properly guided** with structured prompts.

### **What this research explores**
- **Implementation of a multi-stage prompt refinement system** combining:  
  ‚úÖ **Rule-based template generation**  
  ‚úÖ **Dynamic meta-prompting techniques**  
  ‚úÖ **Stateful chaining of prompts** for iterative improvement  
  ‚úÖ **Parameter optimization** for different requirement analysis categories  
  ‚úÖ **Evaluation of Level-1 (meta-prompt) and Level-2 (chained prompting) techniques**

### **Implications for Practice**
- **Automated, high-quality requirements extraction** with **minimal manual intervention**.
- **Standardized, template-based prompt refinement** leading to **more structured outputs**.
- **Improved requirements elicitation** across **diverse domains**.
- **Enhanced consistency and accuracy** in **LLM-driven requirement analysis.**

---

# **Research Methodology**
The project implements a **multi-step prompt refinement system** tested under various configurations.

### **üìå Framework Overview**
The implemented framework consists of the following **core components**:

1Ô∏è‚É£ **Prompt Refiner:**  
- Iteratively improves prompts through **automated quality scoring** and **LLM-based refinements.**  
- Implements a **feedback loop** to ensure continuous optimization.  

2Ô∏è‚É£ **Template Generator:**  
- Dynamically selects **the best prompt template** based on the detected **requirement type, role, and technique.**  

3Ô∏è‚É£ **Analysis Module:**  
- Evaluates **LLM outputs** using **qualitative and quantitative scoring.**  
- Parses **structured requirements** for key quality attributes.  

4Ô∏è‚É£ **Parameter Optimizer:**  
- Experiments with **temperature, context length, and response size** to determine optimal configurations.  

5Ô∏è‚É£ **Experimental Setup:**  
- Tests various **prompt engineering techniques** across multiple requirement analysis tasks.
- **Compares Level-0, Level-1, and Level-2 techniques.**

### **Test Cases**
The experiments evaluate **eight different requirements analysis tasks**, including:
- **Requirements elicitation**
- **Requirements validation**
- **Non-functional requirements identification**
- **Stakeholder needs analysis**
- **Requirements conflict resolution**

---

# **Results & Analysis**

### **üìä Key Findings**
‚úÖ **Level-1 (meta-prompting) and Level-2 (chained prompting) techniques outperform standard approaches.**  
‚úÖ **Stepwise refinement (3-step chains) improves requirement clarity and structure.**  
‚úÖ **Temperature tuning significantly affects quality, with 0.5 yielding optimal results.**  
‚úÖ **Role-based prompting improves extraction accuracy but suffers from misclassification issues.**  
‚úÖ **Automated prompt refinement achieves consistent gains, particularly for complex requirements.**  

### **üìâ Performance Evaluation**
#### 1Ô∏è‚É£ **Technique Effectiveness**
- **Role-Playing & Chain-of-Thought outperform standard zero-shot prompting.**
- **Level-2 Techniques showed the highest average quality score (0.87).**

#### 2Ô∏è‚É£ **Parameter Impact**
- **Temperature = 0.5 consistently produced the most balanced results.**
- **Expanding the context window beyond 2048 tokens had **no significant benefit** in requirements tasks.  

#### 3Ô∏è‚É£ **Prompt Refinement Progression**
- **3 iterations** were optimal; increasing to **5 degraded quality** due to **over-refinement.**
- **Divergent-Convergent techniques performed best for ambiguous requirements.**
- **Adversarial prompting improved conflict detection in requirements documents.**

---

# **Implementation Details**
The repository contains **modularized implementations** for **prompt engineering, experimentation, and reporting.**

### **üìÇ Project Structure**
```
üìÅ prompt/                     # Prompt refinement modules
 ‚îú‚îÄ‚îÄ prompt_refiner.py        # Main prompt refiner system
 ‚îú‚îÄ‚îÄ template_generator.py     # Dynamic prompt template selection
 ‚îú‚îÄ‚îÄ techniques.py             # Implementation of Level-0, Level-1, and Level-2 techniques
 ‚îú‚îÄ‚îÄ analyzers.py              # Output quality analysis and scoring
 ‚îú‚îÄ‚îÄ parameters.py             # Optimization of LLM parameters
 ‚îú‚îÄ‚îÄ utils.py                  # Utility functions

üìÅ research/                    # Experimentation framework
 ‚îú‚îÄ‚îÄ experiments/              # Test cases and configurations
 ‚îú‚îÄ‚îÄ reporting/                # Research reporting tools
 ‚îú‚îÄ‚îÄ visualization.py          # Graph generation for research results
 ‚îú‚îÄ‚îÄ framework.py              # Experimentation framework controller
```

---

# **Further Research & Next Steps**
üîé **Extending Automated Requirements Traceability**  
üìä **Adaptive Prompt Optimization for Different Models**  
ü§ñ **Hybrid Human-LLM Collaboration for Requirements Validation**  
üöÄ **Domain-Specific Adaptations for Regulated Industries**  

---

# **How to Use This Project**
### **1Ô∏è‚É£ Install Dependencies**
```
pip install -r requirements.txt
```

### **2Ô∏è‚É£ Run Experiments**
```
python research_runner.py
```

### **Re-Generate Research Report if it is Missing**
```
python report_regenerator.py
```
### **Running Custom Experiments**

##  **Custom Experiment Runner**
- `custom_experiment.py`: Lets you run experiments with specific test case categories or custom test cases
- Supports command-line arguments for full customization
- Can generate LaTeX reports as part of the experiment

```bash
# Run with standard test cases
python custom_experiment.py --standard

# Run with technical test cases
python custom_experiment.py --technical

# Run with all test cases
python custom_experiment.py --all

# Run with your own test cases
python custom_experiment.py --custom my_test_cases.json

# Limit the number of test cases
python custom_experiment.py --technical --limit 2

# Specify which techniques to test
python custom_experiment.py --academic --techniques "chain_of_thought,socratic"

# Generate a LaTeX report
python custom_experiment.py --business --latex
```

### **Creating Custom Test Cases**

- `research/test_cases.py`: Contains all test cases organized by category
- Updated `framework.py` to use the separate test cases module
- Added methods to set custom test cases, techniques, and parameters

You can create your own test cases in a JSON file:

```json
[
    {
        "query": "Write a function to find prime numbers",
        "category": "programming",
        "expected_role": "Software Engineer",
        "expected_technique": "chain_of_thought",
        "description": "Prime number algorithm task"
    },
    {
        "query": "Explain quantum computing to a 10-year-old",
        "category": "explanation",
        "expected_role": "Teacher",
        "expected_technique": "socratic",
        "description": "Simplified explanation task"
    }
]
```

---

# **Final Remarks**
This research builds upon the **Prompt Engineering Lab** and expands its capabilities by integrating **multi-level automation techniques for requirements analysis.** The **empirical evaluation** confirms the effectiveness of **meta-prompting and refinement chaining**, contributing to the **advancement of structured LLM-assisted requirements engineering.**  

